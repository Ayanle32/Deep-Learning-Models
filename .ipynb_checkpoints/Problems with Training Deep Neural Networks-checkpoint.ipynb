{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95904c95",
   "metadata": {},
   "source": [
    "If we need to tackle a complex problem such as object detection in high resolution images then it becomes necessary to train deeper neural networks with 10 layers or more and hundreds of neurons in each layer. This introduces the following problems\n",
    "\n",
    "1. Vanishing/Exploding Gradients - Deep neural networks are very prone to the vanishing/exploding gradients.\n",
    "2. Inadequate training data for such a large network and it might be costly to label it.\n",
    "3. High training time.\n",
    "4. Deep neural networks have a large number of parameters which can introduce overfitting especially if the training data is too small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935b358d",
   "metadata": {},
   "source": [
    "# Vanishing/Exploding Gradients Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4959616",
   "metadata": {},
   "source": [
    "In the backpropagation algorithm the algorithm starts at the output layer calculates the error and propagates the error gradient down to the input layer. After this it updates the weight using a Gradient Descent algorithm. Usually the gradients get smaller as the algorithm cascades down the layers and this can cause the lower layers connection weights to be virtually unchanged. This is called the vanishing gradients problem. The opposite can happen where the gradients get bigger causing the algorithm to diverge. This is the exploding gradient problem. In general deep neural networks suffer from unstable gradients causing different layers to learn at different rates.\n",
    "\n",
    "The reasons why the neural networks suffer from the vanishing/exploding was explained in a paper by Xavier Glorot and Yoshua Bengio. The main problem is the combination of the sigmoid activation function and the random normal weight initialisation. This causes the variance of the outputs of each layer to be greater than the variance of its inputs. When the inputs are large the sigmoid function saturates at 1 or 0 where the gradient is very small. In the backpropagation stage this gets even smaller and leaves virtually nothing for the lower layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058c598b",
   "metadata": {},
   "source": [
    "# Glorot and He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dbe10d",
   "metadata": {},
   "source": [
    "In their paper they suggested the following initialisations with the sigmoid activation function\n",
    "\n",
    "1. Normal distribution with mean 0 and variance $\\sigma^2 = \\frac{1}{fan}_{avg}$\n",
    "2. Uniform distribution between -r and +r, with $r = \\sqrt{\\frac{3}{fan_{avg}}}$\n",
    "\n",
    "This initialising is very useful and can speed up training significantly.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Initialisation   |    Activation Functions     |  $\\sigma^2$ \n",
    "    \n",
    "        Glorot        None, Tanh, Logistic, Softmax  1\\fan_avg\n",
    "        \n",
    "        He            ReLU & variants                2/fan_in\n",
    "        \n",
    "        LeCun         SELU                           1/fan_in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6737b933",
   "metadata": {},
   "source": [
    "By default keras uses Glorot initialisation with a uniform distribution. We can use He initialisation when creating a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73ff8e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c1f35c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.0'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c3d36b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Dense name=dense_55, built=False>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer = \"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcdec7d",
   "metadata": {},
   "source": [
    "We can use He initialisation with a uniform distribution and use $fan_{avg}$ rather than $fan_{in}$ in the following way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "61bcd441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Dense name=dense_56, built=False>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "he_avg_init = keras.initializers.VarianceScaling(scale=2, mode='fan_avg', distribution = 'uniform')\n",
    "keras.layers.Dense(10, activation = 'sigmoid', kernel_initializer = he_avg_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a850e285",
   "metadata": {},
   "source": [
    "# Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540ebd29",
   "metadata": {},
   "source": [
    "The ReLU activation function behaves much better than the sigmoid function because it is much faster to compute and does not saturate for positive values.\n",
    "\n",
    "Some problems with the ReLU function is that during some neurons can die when they start outputting zeros only. This usually happens when the learning rate is too high and the weights get tweaked in a way such that the weighted sum of all its inputs are negative for all instances. The ReLU function has a vanishing gradient for negative values and gradient descent has no effect. This can be solved by looking for better variants of the ReLU activation function $\\newline$\n",
    "\n",
    "1. $LeakyReLU_{\\alpha}(z) = max(\\alpha z,z)$ \n",
    "\n",
    "Here $\\alpha$ is the slope for $z < 0$\n",
    "\n",
    "2. Randomized leaky ReLU - here $\\alpha$ is picked randomly in a given range and fixed to an average value during testing.\n",
    "\n",
    "\n",
    "3. Parametric leaky ReLU - here $\\alpha$ is not a hyperparameter but a parameter which can be learned and updated by gradient descent. This activation function outperforms ReLU on large image datasets, but on smaller datasets it runs the risk of overfitting.\n",
    "\n",
    "\n",
    "4. $ELU_{\\alpha}(z) = \\begin{cases} \n",
    "                       \\alpha(\\exp(z) - 1): z < 0 \\\\\\\\\n",
    "                       z                  : z \\geq 0\n",
    "                       \\end{cases}$\n",
    "   \n",
    "   The exponential linear unit (ELU) outperforms all the ReLU variants in their experiments. Training time and performance        during test is also better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0570e8a",
   "metadata": {},
   "source": [
    "## Leaky ReLU in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1bf0fc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_relu = keras.layers.LeakyReLU(negative_slope=0.2)\n",
    "layer = keras.layers.Dense(10, activation=leaky_relu, \n",
    "                            kernel_initializer = 'he_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570b2868",
   "metadata": {},
   "source": [
    "## SELU Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f5525ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(10, activation='selu', kernel_initializer = 'lecun_normal') # with selu we must use lecun_normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c46100",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9894b100",
   "metadata": {},
   "source": [
    "Using He normalisation with the ELU activation function can significantly reduce the vanishing/exploding gradients problem. But the distribution can still change during training. Batch normalisation addresses this issue by normalising the inputs to a layer. The algorithm can be summarised as follows\n",
    "\n",
    "\n",
    "1. $\\Large \\mu_{B} = \\frac{1}{m_B}\\sum_{i = 1}^{m_B} x^{i}$ $\\newline$\n",
    "2. $\\Large \\sigma_{B}^2 = \\frac{1}{m_B}\\sum_{i = 1}^{m_B} \\left(x^{i} - \\mu_B\\right)^2$ $\\newline$\n",
    "3. $\\Large \\hat{x}^i = \\frac{x^i - \\mu_B}{\\sqrt{\\sigma_{B}^{2} + \\epsilon}}$ $\\newline$\n",
    "4. $\\Large  z^i = \\gamma \\hat{x}^i + \\beta$ $\\newline$ $\\newline$\n",
    "\n",
    "1. For each feature of the input in the mini batch, the mean and standard deviation are calculated.\n",
    "2. Each input is then normalised by subtracting the mean and dividing by the standard deviation plus a tiny constant.\n",
    "3. The data is scaled and shifted by learned parameters $\\gamma$ and $\\beta$.\n",
    "4. During the testing stage it uses the final input mean and final input standard deviation which are calculated using exponential moving average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a9b53e",
   "metadata": {},
   "source": [
    "## Implementing Batch Normalization in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6da08ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9d3a62a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_15\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_15\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ flatten_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_30               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_31               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,200</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_32               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_61 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ flatten_10 (\u001b[38;5;33mFlatten\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_30               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)                 │           \u001b[38;5;34m3,136\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_59 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)                 │         \u001b[38;5;34m235,500\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_31               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)                 │           \u001b[38;5;34m1,200\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_60 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m30,100\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_32               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │             \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_61 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m1,010\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">271,346</span> (1.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m271,346\u001b[0m (1.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">268,978</span> (1.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m268,978\u001b[0m (1.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,368</span> (9.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,368\u001b[0m (9.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51793c7",
   "metadata": {},
   "source": [
    "Since a Batch Normalization layer includes one offset parameter per input, we can remove the bias term from\n",
    "the previous layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "39270c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer='he_normal', use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6c986b",
   "metadata": {},
   "source": [
    "# Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61185f29",
   "metadata": {},
   "source": [
    "Gradient clipping reduces exploding gradients by restricting the gradient to not exceed some threshold. It is used mostly in RNNs since batch normalisation is difficult to implement in RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "dd91fc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "model_.compile(loss='mse', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd20d972",
   "metadata": {},
   "source": [
    "This will clip every component of the gradient to a value between -1.0 and 1.0. This can usually change the orientation of the gradient vector so usually clipping the norm by setting clipnorm instead of clipvalue maybe preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3895e4bd",
   "metadata": {},
   "source": [
    "# Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e70b4d",
   "metadata": {},
   "source": [
    "Instead of building a large neural network from scratch it can be useful to find an exisiting neural network that performs a similar task and reuse the lower layers of this network. This is called transfer learning. In most cases the output layer should be discarded since it is most likely not relevant to the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad187e0d",
   "metadata": {},
   "source": [
    "# Transfer Learning With Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3f7a53a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.load_model(\"model.keras\") # loading a saved model\n",
    "model_A_clone = keras.models.clone_model(model_A) # clone the model\n",
    "model_A_clone.set_weights(model_A.get_weights()) # clone weights since cloning does not clone weights\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1]) # create a new model B with all the layers of A \n",
    "                                                            # except for the output layer\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\")) # adding a different output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066d1bbd",
   "metadata": {},
   "source": [
    "Now we could use this new model to train for some new task B but one issue is the new output layer is randomly initialised and will cause large error gradients. A solution is to freeze the reused layers and train the output layer for a few epochs in order for it to learn reasonable weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "667c3cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion = keras.datasets.fashion_mnist\n",
    "(X_train, y_train), (X_test, y_test) = fashion.load_data()\n",
    "\n",
    "y_train_B = ((y_train == 5) | (y_train == 6)).astype(int)\n",
    "y_test_B = ((y_test == 5) | (y_test == 6)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ff3c4be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]: # set all but the output layer to non-trainable\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b791d626",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer='sgd', \n",
    "                     metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "21fe2313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 632us/step - accuracy: 0.8760 - loss: 366.9988\n",
      "Epoch 2/4\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 637us/step - accuracy: 0.9068 - loss: 198.5783\n",
      "Epoch 3/4\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 633us/step - accuracy: 0.9057 - loss: 214.2549\n",
      "Epoch 4/4\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 625us/step - accuracy: 0.9079 - loss: 194.9447\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train, y_train_B, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "59890cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "762270c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 797us/step - accuracy: 0.8594 - loss: 190.9481\n",
      "Epoch 2/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 801us/step - accuracy: 0.8843 - loss: 1.4587\n",
      "Epoch 3/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 843us/step - accuracy: 0.8863 - loss: 0.8058\n",
      "Epoch 4/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 803us/step - accuracy: 0.8819 - loss: 0.5851\n",
      "Epoch 5/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 816us/step - accuracy: 0.8916 - loss: 0.4811\n",
      "Epoch 6/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 798us/step - accuracy: 0.8938 - loss: 0.4106\n",
      "Epoch 7/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 820us/step - accuracy: 0.9031 - loss: 0.3602\n",
      "Epoch 8/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 803us/step - accuracy: 0.9055 - loss: 0.3459\n",
      "Epoch 9/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 804us/step - accuracy: 0.9099 - loss: 0.2939\n",
      "Epoch 10/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 799us/step - accuracy: 0.9111 - loss: 0.2640\n",
      "Epoch 11/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 814us/step - accuracy: 0.9108 - loss: 0.2721\n",
      "Epoch 12/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 800us/step - accuracy: 0.9131 - loss: 0.2509\n",
      "Epoch 13/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 803us/step - accuracy: 0.9157 - loss: 0.2546\n",
      "Epoch 14/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 806us/step - accuracy: 0.9178 - loss: 0.2469\n",
      "Epoch 15/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 808us/step - accuracy: 0.9179 - loss: 0.2334\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=1e-4)\n",
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = model_B_on_A.fit(X_train, y_train_B, epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "14215f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634us/step - accuracy: 0.9099 - loss: 0.6440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7545680403709412, 0.9068999886512756]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879cee3e",
   "metadata": {},
   "source": [
    "# Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef220e84",
   "metadata": {},
   "source": [
    "Another way to speed up training neural networks is to use a faster optimizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8c6e13",
   "metadata": {},
   "source": [
    "## Momentum Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b228866",
   "metadata": {},
   "source": [
    "Momentum optimization introduces a momentum term which keep track of previous gradients. This allows for the algorithm to converge faster.\n",
    "\n",
    "1. $\\bf{m} \\leftarrow \\beta \\bf{m} - \\eta \\nabla_{\\theta}J(\\bf{\\theta})$\n",
    "2. $\\bf{\\theta} \\leftarrow \\bf{\\theta} + \\bf{m}$\n",
    "\n",
    "Introduce a momentum term that combines previous gradients with current gradient weighted by $\\bf{m}$. We then update the weights by adding this momentum term to move the weights in the direction of this momentum.\n",
    "\n",
    "Usually $\\beta$ is set to 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c0496e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate = 0.001, momentum = 0.9) # keras implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de251aa0",
   "metadata": {},
   "source": [
    "## Nesterov Accelerated Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbd7d12",
   "metadata": {},
   "source": [
    "Nesterov Accelerated Gradient is slightly different to vanilla Momentum Optimization. The gradient is measured slightly ahead of the local position in the direction of the momentum.\n",
    "\n",
    "1. $\\bf{m} \\leftarrow \\beta \\bf{m} - \\eta \\nabla_{\\theta}J(\\bf{\\theta + \\beta m})$\n",
    "2. $\\bf{\\theta} \\leftarrow \\bf{\\theta} + \\bf{m}$\n",
    "\n",
    "NAG is almost always faster than regular Momentum Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12aa4156",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAG_optimizer = keras.optimizers.SGD(learning_rate = 0.001, momentum = 0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ddffa2",
   "metadata": {},
   "source": [
    "## AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4103c7",
   "metadata": {},
   "source": [
    "1. $ \\bf{s} \\leftarrow \\bf{s} + \\nabla_{\\theta}J(\\bf{\\theta}) \\otimes \\nabla_{\\theta}J(\\bf{\\theta})$\n",
    "2. $ \\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta}J(\\bf{\\theta}) \\oslash \\sqrt{\\bf{s} + \\epsilon}$\n",
    "\n",
    "The first step accumulates the square of the gradients into the vector s. This is equivalent to $s_i \\leftarrow s_i + (\\partial J(\\theta)/\\partial \\theta_i)^2$.\n",
    "\n",
    "The second step scales down the gradient vector by a factor of $\\sqrt{s + \\epsilon}$. This is equivalent to $\\theta_i \\leftarrow \\theta_i - \\eta\\frac{\\partial J(\\theta)/\\partial \\theta_i}{\\sqrt{s_i + \\epsilon}}$ for all parameters $\\theta_i$ simultaneously.\n",
    "\n",
    "This algorithm decays the learning rate faster for steeper dimensions. This prevents overshooting and results in updates more directly towards the global miminum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5d4532",
   "metadata": {},
   "source": [
    "## RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677fdaed",
   "metadata": {},
   "source": [
    "AdaGrad slows down or often stops early when training neural networks. The RMSProp algorithm fixes this by accumulating only the gradients from the most recent iterations.\n",
    "\n",
    "1. $ \\bf{s} \\leftarrow \\beta\\bf{s} + (1 - \\beta)\\nabla_{\\theta}J(\\bf{\\theta}) \\otimes \\nabla_{\\theta}J(\\bf{\\theta})$\n",
    "2. $ \\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta}J(\\bf{\\theta}) \\oslash \\sqrt{\\bf{s} + \\epsilon}$\n",
    "\n",
    "The decay rate $\\beta$ is typically set to 0.9. This among the best and fastest optimizers possibly behind Adam Optimization only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5766a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSProp_optimizer = keras.optimizers.RMSprop(learning_rate = 0.001, rho = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28486702",
   "metadata": {},
   "source": [
    "## Adam & Nadam Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c985e42",
   "metadata": {},
   "source": [
    "Adam Optimization combines the ideas of Momentum Optimization and RMSProp.\n",
    "\n",
    "1. $\\Large \\bf{m} \\leftarrow \\beta_1 \\bf{m} - (1 - \\beta_1)\\nabla_{\\theta}J(\\bf{\\theta})$\n",
    "2. $\\Large \\bf{s} \\leftarrow \\beta_2\\bf{s} + (1 - \\beta_2)\\nabla_{\\theta}J(\\bf{\\theta}) \\otimes \\nabla_{\\theta}J(\\bf{\\theta})$ \n",
    "3. $\\Large \\hat{\\bf{m}} \\leftarrow \\frac{\\bf{m}}{1 - \\beta_{1}^{t}}$\n",
    "4. $\\Large \\hat{\\bf{s}} \\leftarrow \\frac{\\bf{s}}{1 - \\beta_{2}^{t}}$\n",
    "5. $\\Large \\theta \\leftarrow \\theta + \\eta \\hat{\\bf{m}}\\oslash \\sqrt{\\hat{\\bf{s}} + \\epsilon}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f22a4d8",
   "metadata": {},
   "source": [
    "steps 1 is slightly different from Momentum Optimization as it computes a decaying average rather than a decaying sum and these are equivalent upto a factor of $(1 - \\beta_1)$. Steps 3 and 4 boost $\\bf{m}$ and $\\bf{s}$ at the begining as they are initialised to 0 and will be biased towards zero at the start of training.\n",
    "\n",
    "The hyperparameter $\\beta_1$ is typically set to 0.9 and $\\beta_2$ is typically set to 0.999. $\\epsilon$ is a smoothing term typically set to a small number $10^{-7}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0c1e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adam_optimizer = keras.optimizers.Adam(learning_rate = 0.001, beta_1 = 0.9, beta_2 = 0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ed6ac2",
   "metadata": {},
   "source": [
    "Adam is an adaptive learning rate algorithm and requires less tuning of the learning rate hyperparameter $\\eta$. We can use the default value of 0.001."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f013b2",
   "metadata": {},
   "source": [
    "### Variants of the Adam Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b31479f",
   "metadata": {},
   "source": [
    "Adamax - This variant replaces step 2 with $s \\leftarrow max(\\beta_2 \\bf{s}, \\nabla_\\theta J(\\theta))$, it drops step 4 and in step 5 it scales down the gradient updates by a factor of $\\bf{s}$. This can provide more stability in certain task over using Adam but generally it performs worse.\n",
    "\n",
    "Nadam Optimization - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4325fd04",
   "metadata": {},
   "source": [
    "# Avoiding Overfitting Through Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2bf5b2",
   "metadata": {},
   "source": [
    "Since neural networks have a huge number of parameters and therefore an incredible amount of degrees of freedom. This calls for the need of regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edf70a5",
   "metadata": {},
   "source": [
    "## $\\mathcal{l}_1$ and $\\mathcal{l}_2$ Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e624cce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation='elu', \n",
    "                           kernel_initializer='he_normal',\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b813853b",
   "metadata": {},
   "source": [
    "Applying regularization to all hidden layers can become tedious and is error-prone. To avoid this we can use functools.partial() function which allows us to create thin wrapper for any callable with default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e84b9c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cca9faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "RegularizedDense = partial(keras.layers.Dense, \n",
    "                           activation='elu',\n",
    "                           kernel_initializer='he_normal',\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7886d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Input([28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\",\n",
    "                     kernel_initializer=\"glorot_uniform\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78275d7f",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa73e429",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
