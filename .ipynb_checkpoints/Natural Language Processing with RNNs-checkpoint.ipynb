{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ee18711",
   "metadata": {},
   "source": [
    "# Generating Shakespearean Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239156c9",
   "metadata": {},
   "source": [
    "## Creating the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "91c1018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "417f5794",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://homl.info/shakespeare\"\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", url,\n",
    "                               cache_dir = '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3da7fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath) as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbca169",
   "metadata": {},
   "source": [
    "We have now downloaded the dataset. Now we need to encode every character as an integer. To do this we can use Keras Tokenizer class which maps every character in the text to a unique character id from 1 to the number of distinct characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fb2f9632",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01b2e55",
   "metadata": {},
   "source": [
    "Here set char_level = True to get character level encoding instead of the default word-level encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0395a0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12, 5, 8, 3]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['last'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "36d4e2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l a s t']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[12, 5, 8, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "050637c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_ids = len(tokenizer.word_index) # number of distinct characters\n",
    "distinct_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "91266fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([text])) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c27d7e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded) # total number of characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328ec660",
   "metadata": {},
   "source": [
    "## How to Split a Sequential Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779dd508",
   "metadata": {},
   "source": [
    "It is very important to avoid any overlap between the training set, the validation set, and the test set. For example we use the first 90% for training the next 5% for validation and the final 5% for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2fdbd251",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(encoded) * 90 // 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "32e20c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a356479a",
   "metadata": {},
   "source": [
    "## Chopping the Sequential Dataset into Multiple Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bb59e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True) # creates a datset of windows and each window is a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "59aa87d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length)) # creates a dataset of tensors of length window_length.\n",
    "                                                                       # This essentially flattens the nested dataset above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "44dcb210",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:])) # separate inputs from target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0feaf125",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=distinct_ids), Y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "90fdcbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a600acbb",
   "metadata": {},
   "source": [
    "# Building and Training the RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dafbaa8",
   "metadata": {},
   "source": [
    "Since we want to predict the next character based on the previous 100 characters, we can use an RNN with GRU layers of 128 units each. The output is a time distributed dense layer with 39 units for each character and a softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f1f99dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, distinct_ids]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(distinct_ids, activation='softmax'))\n",
    "    \n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "51ddbb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9822ce9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "31368/31368 [==============================] - 484s 15ms/step - loss: 0.9620\n",
      "Epoch 2/20\n",
      "31368/31368 [==============================] - 470s 15ms/step - loss: 0.9634\n",
      "Epoch 3/20\n",
      "31368/31368 [==============================] - 467s 15ms/step - loss: 1.0244\n",
      "Epoch 4/20\n",
      "31368/31368 [==============================] - 471s 15ms/step - loss: 1.0730\n",
      "Epoch 5/20\n",
      "31368/31368 [==============================] - 469s 15ms/step - loss: 1.1039\n",
      "Epoch 6/20\n",
      "31368/31368 [==============================] - 474s 15ms/step - loss: 1.1401\n",
      "Epoch 7/20\n",
      "31368/31368 [==============================] - 472s 15ms/step - loss: 1.1650\n",
      "Epoch 8/20\n",
      "31368/31368 [==============================] - 474s 15ms/step - loss: 1.1873\n",
      "Epoch 9/20\n",
      "31368/31368 [==============================] - 473s 15ms/step - loss: 1.2133\n",
      "Epoch 10/20\n",
      "31368/31368 [==============================] - 475s 15ms/step - loss: 1.2313\n",
      "Epoch 11/20\n",
      "31368/31368 [==============================] - 470s 15ms/step - loss: 1.2616\n",
      "Epoch 12/20\n",
      "31368/31368 [==============================] - 470s 15ms/step - loss: 1.2773\n",
      "Epoch 13/20\n",
      "31368/31368 [==============================] - 475s 15ms/step - loss: 1.2686\n",
      "Epoch 14/20\n",
      "31368/31368 [==============================] - 476s 15ms/step - loss: 1.2959\n",
      "Epoch 15/20\n",
      "31368/31368 [==============================] - 474s 15ms/step - loss: 1.3209\n",
      "Epoch 16/20\n",
      "31368/31368 [==============================] - 477s 15ms/step - loss: 1.3183\n",
      "Epoch 17/20\n",
      "31368/31368 [==============================] - 468s 15ms/step - loss: 1.3121\n",
      "Epoch 18/20\n",
      "31368/31368 [==============================] - 472s 15ms/step - loss: 1.3434\n",
      "Epoch 19/20\n",
      "31368/31368 [==============================] - 468s 15ms/step - loss: 1.3538\n",
      "Epoch 20/20\n",
      "31368/31368 [==============================] - 473s 15ms/step - loss: 1.3544\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2d36e281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    " X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    " return tf.one_hot(X, distinct_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e7cd2ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = preprocess([\"how are yo\"])\n",
    "Y_pred = np.argmax(model.predict(X_new), axis=1)\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "89d4d62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 5, 7, 8, 0, 0, 7, 2, 5, 4, 6, 4, 3, 9, 5, 7, 1, 2, 4, 3, 7, 3,\n",
       "       4, 2, 7, 2, 2, 6, 2, 6, 2, 0, 7, 3, 4, 1, 9, 9, 9], dtype=int64)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
