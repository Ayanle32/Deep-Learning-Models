{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6fee9d",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks are extremely useful in natural language processing, time series analysis and even autonomous driving systems.\n",
    "\n",
    "An RNN looks very similar to a feedforward neural network (FNN) but unlike FNNs RNNs also have connections pointing backward.\n",
    "\n",
    "The simplest RNN is composed of 1 neuron that takes some input at time step t $\\bf{x}_{(t)}$ as well as its own output from the previous time step, $y_{(t-1)}$.\n",
    "\n",
    "With a layer of neurons at each time step t, every neuron receives an input vector $\\bf{x}_t$ and the output vector from the previous time step $\\bf{y}_{(t-1)}$.\n",
    "\n",
    "Each neuron has two sets of weights: $\\bf{w}_x$ and $\\bf{w}_y$ for the input $\\bf{x}_{(t)}$ and $\\bf{y}_{(t - 1)}$. For the whole network these two weights can be place in two weight matrices, $\\bf{W}_x$ and $\\bf{W}_y$. The output vector for the entire layer can be computed as $\\newline$\n",
    "\n",
    "$\\Large \\bf{y}_{(t)} = \\phi\\left(\\bf{W}_{x}^T\\bf{x}_{(t)} + \\bf{W}_{y}^T\\bf{y}_{(t - 1)} + \\bf{b} \\right)$\n",
    "\n",
    "this is the output for a single instance. For a mini-batch we have\n",
    "\n",
    "$\\Large \\bf{Y}_{(t)} = \\phi\\left(\\bf{X}_{(t)}\\bf{W}_{x} + \\bf{Y}_{(t - 1)}\\bf{W}_{y} + \\bf{b} \\right)$ $\\newline$\n",
    "                     $ \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\,\\Large = \\phi\\left(\\bigl[\\bf{X}_{(t)} \\bf{Y}_{(t - 1)}\\bigr]\\bf{W} + \\bf{b} \\right)$\n",
    "                     \n",
    "Where $\\Large \\bf{W} = \\begin{bmatrix}\n",
    "W_x \\\\\n",
    "W_y\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\newline$\n",
    "\n",
    "\n",
    "1. $\\bf{Y}_{(t)}$\n",
    " is an $m \\times n_{neurons}$ matrix containing the layer’s outputs at time step t for each\n",
    "instance in the mini-batch (m is the number of instances in the mini-batch and\n",
    "nneurons is the number of neurons).\n",
    "\n",
    "\n",
    "2. $X_{(t)}$\n",
    " is an $m \\times n_{inputs}$ matrix containing the inputs for all instances ($n_{inputs}$ is the\n",
    "number of input features).\n",
    "\n",
    "\n",
    "3. $W_x$\n",
    " is an ninputs × nneurons matrix containing the connection weights for the inputs\n",
    "of the current time step.\n",
    "\n",
    "\n",
    "4. $W_y$\n",
    " is an nneurons × nneurons matrix containing the connection weights for the out‐\n",
    "puts of the previous time step.\n",
    "\n",
    "\n",
    "5. $\\bf{b}$ is a vector of size nneurons containing each neuron’s bias term.\n",
    "\n",
    "\n",
    "6. The weight matrices $W_x$\n",
    " and $W_y$\n",
    " are often concatenated vertically into a single\n",
    "weight matrix $\\bf{W}$ of shape $(n_{inputs} + n_{neurons}) \\times n_{neurons}$.\n",
    "\n",
    "\n",
    "7. The notation $[X_{(t)} Y_{(t–1)}]$ represents the horizontal concatenation of the matrices\n",
    "$X_{(t)}$\n",
    " and $Y_{(t–1)}$.\n",
    " \n",
    "\n",
    "\n",
    "It should be noted that $Y_{(t)}$ is a function of $X_{(t)}$  and $Y_{(t-1)}$ which is a function of $X_{(t - 1)}$  and $Y_{(t-2)}$, which is a function of $X_{(t - 2)}$  and $Y_{(t-3)}$ and so on. At the first time step these are all zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7296eda2",
   "metadata": {},
   "source": [
    "# Memory Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bac744",
   "metadata": {},
   "source": [
    "We could say that a recurrent neuron has a form of memory since the output is a function of all the inputs from previous time steps. A single neuron, or a layer of recurrent neurons is an example of a basic memory cell, capable of learning short patterns that are typically 10 steps long.\n",
    "\n",
    "In general a cell's state at time step t, $h_{(t)}$ is a function of some inputs at that time step and its state the previous time step: $h_{(t)} = f(h_{(t-1)}, x_{(t)})$. It's output at time step t, $y_{(t)}$ is also a function of the previous state and current inputs. In the cases discussed so far the state and output are equal but this might not always be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6a196f",
   "metadata": {},
   "source": [
    "# Input and Output Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca04506",
   "metadata": {},
   "source": [
    "An RNN can take a sequence of inputs and produce a sequence of outputs. For example it can take stock prices over the last N days and then it outputs stock prices from N-1 days ago to tomorrow. This type of network is called a sequence-to-sequence network and is useful for predicting time series data.\n",
    "\n",
    "We can also take a sequence of inputs and ignore all outputs except for the last one. For example we could feed the network a sequence of words corresponding to a movie review, and the network could ouput a sentiment score. This type of network is called a sequence-to-vector network.\n",
    "\n",
    "We also have vector-to-sequence networks. Here we feed the network the same input vector over and over again and let it output a sequence. For example the input could be an image (or the output of a CNN), and the output could be a caption for that image.\n",
    "\n",
    "Another possibility is we first use a sequence-to-vector network called an encoder, followed by a vector-to-sequence network called a decoder. We can feed the network a sentence in one language, convert this to a vector representation and then the decoder would decode this vector into a sentence in another language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306764f5",
   "metadata": {},
   "source": [
    "# Training RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3529e1ee",
   "metadata": {},
   "source": [
    "Like regular backpropagation, there is a first forward pass. Here the current inputs and aswell as the outputs from the previous timestamp are fed into the network. The output sequence is evaluated using a cost function. The gradients of this cost function are then propagated backward through the unrolled network. The model parameters are then updated using the gradients.\n",
    "In certain cases the cost function may ignore some outputs and in this case the gradients flow through only the outputs that were used to compute the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5315a682",
   "metadata": {},
   "source": [
    "# Forecasting a Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0653137d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ayanle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8deb255",
   "metadata": {},
   "source": [
    "We will create a function that produces as many series as requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8b3160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series(batch_size, n_steps):\n",
    " freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
    " time = np.linspace(0, 1, n_steps)\n",
    " series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10)) # wave 1\n",
    " series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2\n",
    " series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5) # + noise\n",
    " return series[..., np.newaxis].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b9ee1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 50\n",
    "series = generate_time_series(10000, n_steps + 1) # 10,000 series each with 50 time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e1af90",
   "metadata": {},
   "source": [
    "## Creating Training, Test and Validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0408525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
    "X_val, y_val = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14a4722d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a05811b",
   "metadata": {},
   "source": [
    "In X_train we have 7000 series each with 50 time steps and y_train is the target vector which contains the last or the 51st time step from the original series. So essentially we will try to predict this value based on the previous 50 time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159a2aca",
   "metadata": {},
   "source": [
    "# Baseline Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16d1cc8",
   "metadata": {},
   "source": [
    "## Naive Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca6c3f5",
   "metadata": {},
   "source": [
    "This is a basic model that just predicts the last value in the series. So all it does is given the last 50 time steps it just predicts the last value in the series as the 51st value. This is a baseline metric that we can use to see if our RNN model is doing better than a basic model such as this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f72d99f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019714875"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = X_val[:, -1]\n",
    "np.mean(keras.losses.mse(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be4923",
   "metadata": {},
   "source": [
    "## Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc07bc08",
   "metadata": {},
   "source": [
    "In this case we fit a simple linear regression model to the time series and see how it performance compared to RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bdcac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    " keras.layers.Flatten(input_shape = [50, 1]),\n",
    " keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9707513",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0fded5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "219/219 [==============================] - 1s 877us/step - loss: 0.0558\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 0s 833us/step - loss: 0.0138\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 0s 826us/step - loss: 0.0078\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 0s 909us/step - loss: 0.0063\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 0s 819us/step - loss: 0.0056\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 0s 835us/step - loss: 0.0053\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 0s 911us/step - loss: 0.0050\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 0s 840us/step - loss: 0.0048\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 0s 846us/step - loss: 0.0046\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 0s 863us/step - loss: 0.0044\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 0s 898us/step - loss: 0.0043\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 0s 844us/step - loss: 0.0042\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 0s 824us/step - loss: 0.0041\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 0s 993us/step - loss: 0.0040\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 0s 923us/step - loss: 0.0039\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 0s 818us/step - loss: 0.0039\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 0s 918us/step - loss: 0.0038\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 0s 773us/step - loss: 0.0037\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 0s 808us/step - loss: 0.0037\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 0s 959us/step - loss: 0.0036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ec0e3d9190>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e13c8a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 788us/step - loss: 0.0035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.003515490097925067"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bdaef6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.0335925 ],\n",
       "        [ 0.23121265],\n",
       "        [-0.12124686],\n",
       "        [-0.18946454],\n",
       "        [ 0.3157627 ],\n",
       "        [-0.09711541],\n",
       "        [-0.09314709],\n",
       "        [ 0.10129108],\n",
       "        [ 0.15074727],\n",
       "        [-0.11804308],\n",
       "        [ 0.11568407],\n",
       "        [-0.13561237],\n",
       "        [ 0.04642106],\n",
       "        [-0.04041718],\n",
       "        [ 0.21244538],\n",
       "        [-0.04088664],\n",
       "        [ 0.07193008],\n",
       "        [-0.07880671],\n",
       "        [-0.10231889],\n",
       "        [-0.02343206],\n",
       "        [ 0.19717428],\n",
       "        [-0.12191384],\n",
       "        [ 0.07610077],\n",
       "        [-0.02141514],\n",
       "        [-0.00764074],\n",
       "        [-0.08880735],\n",
       "        [-0.20378035],\n",
       "        [-0.09500337],\n",
       "        [ 0.12794864],\n",
       "        [ 0.16298828],\n",
       "        [-0.10310844],\n",
       "        [-0.23570614],\n",
       "        [-0.18816598],\n",
       "        [-0.16300747],\n",
       "        [ 0.12291079],\n",
       "        [ 0.06325466],\n",
       "        [-0.15044206],\n",
       "        [ 0.02334505],\n",
       "        [-0.14727363],\n",
       "        [-0.16614589],\n",
       "        [-0.28066596],\n",
       "        [ 0.02584621],\n",
       "        [-0.02880346],\n",
       "        [ 0.3056407 ],\n",
       "        [-0.12848346],\n",
       "        [-0.27002862],\n",
       "        [-0.27918625],\n",
       "        [-0.2672594 ],\n",
       "        [ 0.27766797],\n",
       "        [ 0.88893116]], dtype=float32),\n",
       " array([0.00414002], dtype=float32)]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights() # 50 params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee93366",
   "metadata": {},
   "source": [
    "# Implementing a Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "83ddf44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(1, input_shape=([None, 1]))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141e290",
   "metadata": {},
   "source": [
    "Here we have a simple RNN with just one neuron. Here we do not need to specify the length of the input sequence as an RNN can process any number of time steps. The SimpleRNN layers uses the hyperbolic tangent activation function by default. The initial state $h_{(init)}$ is set to 0, and it is passed to a single neuron along with the value of the first time step, $x_{(0)}$. The neuron computes a weighted sum of these values and applies the activation function to the result to give the first output $y_0$. For a SimpleRNN this output is also the new state $h_0$. This new state is passed to the same recurrent neuron along with next input value $x_1$ and the process is repeated until the last time step. The layer just outputs the last value, $y_{49}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "93181a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "65699a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0632\n",
      "Epoch 2/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0409\n",
      "Epoch 3/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0282\n",
      "Epoch 4/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0186\n",
      "Epoch 5/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0141\n",
      "Epoch 6/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0122\n",
      "Epoch 7/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0114\n",
      "Epoch 8/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0117\n",
      "Epoch 9/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0112\n",
      "Epoch 10/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0112\n",
      "Epoch 11/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0115\n",
      "Epoch 12/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0112\n",
      "Epoch 13/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0112\n",
      "Epoch 14/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0111\n",
      "Epoch 15/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0111\n",
      "Epoch 16/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0108\n",
      "Epoch 17/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0116\n",
      "Epoch 18/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0111\n",
      "Epoch 19/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0110\n",
      "Epoch 20/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e243e7cc10>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8c8caa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0103 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.010736643336713314"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eb1eb6",
   "metadata": {},
   "source": [
    "We can see that this Simple RNN outperforms the naive approach and but does not do better than the simple linear regression model. The linear regression model has 50 parameters for the values of the 50 time steps as well as a bias term giving it 51 parameters in total.\n",
    "The SimpleRNN has one parameter per input and per hidden state dimension plus a bias term giving it 3 total parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9ef45122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1.6609677]], dtype=float32),\n",
       " array([[-0.66444606]], dtype=float32),\n",
       " array([0.01183371], dtype=float32)]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights() # 3 params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fc016b",
   "metadata": {},
   "source": [
    "# Deep RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2e0b64ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a8325ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_11\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_11\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ simple_rnn_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">440</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ simple_rnn_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">820</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ simple_rnn_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ simple_rnn_14 (\u001b[38;5;33mSimpleRNN\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)            │             \u001b[38;5;34m440\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ simple_rnn_15 (\u001b[38;5;33mSimpleRNN\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)            │             \u001b[38;5;34m820\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ simple_rnn_16 (\u001b[38;5;33mSimpleRNN\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m22\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,282</span> (5.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,282\u001b[0m (5.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,282</span> (5.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,282\u001b[0m (5.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "72008f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "fc7f9c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0441\n",
      "Epoch 2/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0048\n",
      "Epoch 3/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0036\n",
      "Epoch 4/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0033\n",
      "Epoch 5/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0032\n",
      "Epoch 6/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0030\n",
      "Epoch 7/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0032\n",
      "Epoch 8/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0030\n",
      "Epoch 9/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0030\n",
      "Epoch 10/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0030\n",
      "Epoch 11/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0030\n",
      "Epoch 12/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0029\n",
      "Epoch 13/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0029\n",
      "Epoch 14/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0028\n",
      "Epoch 15/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0030\n",
      "Epoch 16/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0029\n",
      "Epoch 17/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0029\n",
      "Epoch 18/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0028\n",
      "Epoch 19/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0029\n",
      "Epoch 20/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e253ef8ac0>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "450831fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0027923425659537315"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600f9ee0",
   "metadata": {},
   "source": [
    "This outperforms all the models we have considered so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c44f2a5",
   "metadata": {},
   "source": [
    "# Forecasting Several Times Steps Ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "df849221",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_time_series(10000, n_steps + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "964383a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\n",
    "X_val, y_val = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000:, -10:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "395aa4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20),\n",
    "    keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b127f6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "38d66065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1288\n",
      "Epoch 2/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0310\n",
      "Epoch 3/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0199\n",
      "Epoch 4/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0156\n",
      "Epoch 5/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0132\n",
      "Epoch 6/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0127\n",
      "Epoch 7/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0121\n",
      "Epoch 8/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0108\n",
      "Epoch 9/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0116\n",
      "Epoch 10/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0105\n",
      "Epoch 11/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0096\n",
      "Epoch 12/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0099\n",
      "Epoch 13/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0094\n",
      "Epoch 14/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0093\n",
      "Epoch 15/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0096\n",
      "Epoch 16/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0100\n",
      "Epoch 17/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0091\n",
      "Epoch 18/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0088\n",
      "Epoch 19/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0088\n",
      "Epoch 20/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e25513bf10>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d05d9746",
   "metadata": {},
   "outputs": [],
   "source": [
    "series2 = generate_time_series(1, n_steps + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3f016949",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new, Y_new = series2[:, :n_steps], series2[:, n_steps:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0915c4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a9933eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0086 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.008478774689137936"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4621bfee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0028288208"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(keras.losses.mse(Y_new, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "259d2830",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.empty((10000, n_steps, 10)) # each target is a sequence of 10D vectors\n",
    "for step_ahead in range(1, 10 + 1):\n",
    " Y[:, :, step_ahead - 1] = series[:, step_ahead:step_ahead + n_steps, 0]\n",
    "Y_train = Y[:7000]\n",
    "Y_valid = Y[7000:9000]\n",
    "Y_test = Y[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "8a62e094",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5c977b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "26b45737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.0473\n",
      "Epoch 2/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0284\n",
      "Epoch 3/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0265\n",
      "Epoch 4/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0260\n",
      "Epoch 5/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0241\n",
      "Epoch 6/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0227\n",
      "Epoch 7/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0213\n",
      "Epoch 8/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0208\n",
      "Epoch 9/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0203\n",
      "Epoch 10/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0205\n",
      "Epoch 11/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0195\n",
      "Epoch 12/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0200\n",
      "Epoch 13/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0195\n",
      "Epoch 14/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0192\n",
      "Epoch 15/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0190\n",
      "Epoch 16/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0191\n",
      "Epoch 17/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0192\n",
      "Epoch 18/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0183\n",
      "Epoch 19/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0183\n",
      "Epoch 20/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e255126df0>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4425d018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.018122989684343338"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, Y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6499be",
   "metadata": {},
   "source": [
    "# The Unstable Gradients Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0de552",
   "metadata": {},
   "source": [
    "We can deploy the same tricks we used in deep neural networks to alleviate the vanishing/exploding gradients problems in RNNs aswell. Good initialization, faster optimizers, dropout, etc. Non-saturating activation function like ReLU don't help much because they can cause gradients to explode and so saturating activation functions like tanh are used to stabilise the gradients. The risk of exploding gradients can also be reduced by using smaller learning rates or gradient clipping.\n",
    "\n",
    "One form of normalization that works well with RNNs is layer normalization. It is very similar to Batch Normalization, but instead of normalizing across the batch dimension, it normalizes across the features dimension. One advantage is that it can compute the required statistics on the fly, at each time step, independently for each instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f86580e",
   "metadata": {},
   "source": [
    "## Implementing Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8b4f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LNSimpleRNNCell(keras.layers.Layer):\n",
    "    def __init__(self, units, activation='tanh', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.state_size = units\n",
    "        self.output_size = units\n",
    "        self.simple_rnn_cell = keras.layers.SimpleRNNCell(units, activation=None)\n",
    "        self.layer_norm = keras.layers.LayerNormalization()\n",
    "        self.activation = keras.activations.get(activation)\n",
    "    def call(self, inputs, states):\n",
    "        outputs, new_states = self.simple_rnn_cell(inputs, states)\n",
    "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
    "        return norm_outputs, [norm_outputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e874b14c",
   "metadata": {},
   "source": [
    "The LNSimpleRNNCell inherits from the keras.layers.Layer class just like any customer layer. The constructor takes the number of units and the desired activation function and sets the state size and output size. Both of these are equal to the number of units for simplernn. We then create a simple rnn cell with no activation function because we want to perform layer normalization after the linear operation but before the activation function. We then create the LayerNormalization layer and fetch the activation function.\n",
    "\n",
    "In the call() method applies the SimpleRNN cell and computes a linear combination of the current inputs and previous hidden states. It then returns the result twice as outputs = new_states[0]. Next we apply Layer Normalization followed by the activation function.\n",
    "\n",
    "Similarly, you could create a custom cell to apply dropout between each time step. But there’s a simpler way: all recurrent layers (except for keras.layers.RNN) and all cells provided by Keras have a dropout hyperparameter and a recurrent_dropout\n",
    "hyperparameter: the former defines the dropout rate to apply to the inputs (at each time step), and the latter defines the dropout rate for the hidden states (also at each time step). No need to create a custom cell to apply dropout at each time step in an RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b547a",
   "metadata": {},
   "source": [
    "# The Short-Term Memory Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13ade7a",
   "metadata": {},
   "source": [
    "After all the transformation the data goes through when traversing the RNN, some information is lost at each time step. This means after some time, the RNN's hidden state contains virtually no trace of the initial inputs. To tackle this problem various types of long-term memory cells have been developed making SimpleRNN obsolete the most popular one being the LSTM cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5ca83f",
   "metadata": {},
   "source": [
    "## LSTM Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39835bc9",
   "metadata": {},
   "source": [
    "The LSTM cell can be used just like a basic cell but it will perform better, training will converge faster, and also it will detect long-term dependecies in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "963dcf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.LSTM(20, return_sequences = True, input_shape=[None, 1]),\n",
    "    keras.layers.LSTM(20, return_sequences = True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0499861f",
   "metadata": {},
   "source": [
    "The state of an LSTM is split into two vector $\\bf{h}_{(t)}$ and $\\bf{c}_{(t)}$ which are short-term state and long-term state respectively.\n",
    "\n",
    "The long-term state $\\bf{c}_{(t-1)}$ first goes through a forget gate, dropping some memories, then go through the addition operation adding some new memories. Then it goes through. However, at the addition operation, it is copied and passed through tanh and then filtered by the output gate. This produces the short-term state $\\bf{h}_{(t)}$.\n",
    "\n",
    "The current input and the previous short-term state $\\bf{x}_{(t)}$, $\\bf{h}_{(t)}$ are fed to four different fully connected layers.\n",
    "\n",
    "1. The main layer outputs $\\bf{g}_{(t)}$ which has the usual role of analysing the current inputs and the previous short-term state. The most important parts of this output are stored in the long-term state and the rest is dropped.\n",
    "\n",
    "2. The other three layers are gate controllers. They use the logistic activation function with output range from 0 to 1. The outputs are fed to element-wise multiplication operations. If they output 0s they close the gate and if they output 1s they open it.\n",
    "\n",
    "- The forget gate controls which parts of the long-term state should be erased.\n",
    "- The input gate controls which parts of $\\bf{g}_{(t)}$ should be added to the long-term state.\n",
    "- The output gate controls which parts of the long-term state should be read and output at this time step, both to         $\\bf{h}_{(t)}$ and to $\\bf{y}_{(t)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256bbc0e",
   "metadata": {},
   "source": [
    "## LSTM computations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e392fb",
   "metadata": {},
   "source": [
    "$ \\Large i_{(t)} = \\sigma\\left(W_{xi}^{T} x_{(t)} + W_{hi}^{T}h_{(t-1)} + b_i\\right)$ $\\newline$\n",
    "$ \\Large f_{(t)} = \\sigma\\left(W_{xf}^{T} x_{(t)} + W_{hf}^{T}h_{(t-1)} + b_f\\right)$ $\\newline$\n",
    "$ \\Large o_{(t)} = \\sigma\\left(W_{xo}^{T} x_{(t)} + W_{ho}^{T}h_{(t-1)} + b_o\\right)$ $\\newline$\n",
    "$ \\Large g_{(t)} = \\tanh\\left(W_{xg}^{T} x_{(t)} + W_{hg}^{T}h_{(t-1)} + b_g\\right)$ $\\newline$\n",
    "$ \\Large c_{(t)} = f_{(t)} \\otimes c_{(t-1)} + i_{(t)} \\otimes g_{(t)}$ $\\newline$\n",
    "$ \\Large y_{(t)} = h_{(t)} = o_{(t)} \\otimes \\tanh(c_{(t)}$\n",
    "\n",
    "In this equation:\n",
    "\n",
    "• $W_{xi}$, $W_{xf}$, $W_{xo}$, $W_{xg}$ are the weight matrices of each of the four layers for their connection to the input vector $x_{(t)}$.\n",
    "\n",
    "\n",
    "• $W_{hi}$, $W_{hf}$, $W_{ho}$, and $W_{hg}$ are the weight matrices of each of the four layers for their\n",
    "connection to the previous short-term state $h_{(t–1)}$.\n",
    "\n",
    "\n",
    "• $b_i$, $b_f$, $b_o$, and $b_g$ are the bias terms for each of the four layers. Note that TensorFlow initializes bf to a vector full of 1s instead of 0s. This prevents forgetting everything at the beginning of training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf1f91e",
   "metadata": {},
   "source": [
    "## Peephole Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb0fdcd",
   "metadata": {},
   "source": [
    "In a regular LSTM the gate controllers can only look at the current input $x){(t)}$ and the previous short-term state $h_{(t-1)}$. But we can also feed it the previous long-term state $c_{(t-1)}$ for some extra context, this is fed to the forget gate and the input gate. The current long-term state $c_{(t)}$ is added as an input to the the controller of the output gate. This usually improves performance but not always. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b64ee4b",
   "metadata": {},
   "source": [
    "## GRU Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a63818",
   "metadata": {},
   "source": [
    "The GRU cell is a simplified version of the LSTM cell, and it performs similary. It works in the following way\n",
    "\n",
    "Both states are merged into a single vector $h_{(t)}$.\n",
    "\n",
    "A single gate controller $z_{(t)}$ controls both the forget gate and the input gate. If the gate controller outputs a 1, the forget gate is open and the input gate is closed and vice versa.\n",
    "\n",
    "There is no output gate. However, there is a new gate controller $r_{(t)}$ that controls which part of the previous state will be shown to the main layer $g_{(t)}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ce84f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
