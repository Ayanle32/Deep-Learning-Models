{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ee18711",
   "metadata": {},
   "source": [
    "# Generating Shakespearean Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239156c9",
   "metadata": {},
   "source": [
    "## Creating the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91c1018d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ayanle\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "417f5794",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://homl.info/shakespeare\"\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", url,\n",
    "                               cache_dir = '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3da7fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath) as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbca169",
   "metadata": {},
   "source": [
    "We have now downloaded the dataset. Now we need to encode every character as an integer. To do this we can use Keras Tokenizer class which maps every character in the text to a unique character id from 1 to the number of distinct characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb2f9632",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01b2e55",
   "metadata": {},
   "source": [
    "Here set char_level = True to get character level encoding instead of the default word-level encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0395a0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12, 5, 8, 3]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['last'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36d4e2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l a s t']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[12, 5, 8, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "050637c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_ids = len(tokenizer.word_index) # number of distinct characters\n",
    "distinct_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91266fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([text])) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c27d7e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded) # total number of characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328ec660",
   "metadata": {},
   "source": [
    "## How to Split a Sequential Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779dd508",
   "metadata": {},
   "source": [
    "It is very important to avoid any overlap between the training set, the validation set, and the test set. For example we use the first 90% for training the next 5% for validation and the final 5% for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fdbd251",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(encoded) * 90 // 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32e20c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a356479a",
   "metadata": {},
   "source": [
    "## Chopping the Sequential Dataset into Multiple Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb59e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True) # creates a datset of windows and each window is a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59aa87d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length)) # creates a dataset of tensors of length window_length.\n",
    "                                                                       # This essentially flattens the nested dataset above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44dcb210",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:])) # separate inputs from target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0feaf125",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=distinct_ids), Y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90fdcbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a600acbb",
   "metadata": {},
   "source": [
    "# Building and Training the RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dafbaa8",
   "metadata": {},
   "source": [
    "Since we want to predict the next character based on the previous 100 characters, we can use an RNN with GRU layers of 128 units each. The output is a time distributed dense layer with 39 units for each character and a softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1f99dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, distinct_ids]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(distinct_ids, activation='softmax'))\n",
    "    \n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51ddbb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9822ce9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  15385/Unknown - 314s 19ms/step - loss: 0.9869"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d36e281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    " X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    " return tf.one_hot(X, distinct_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cd2ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = preprocess([\"how are yo\"])\n",
    "Y_pred = np.argmax(model.predict(X_new), axis=1)\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d4d62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
